<div id=toc></div>

# Table of Contents

- [eess.IV](#eess.IV) [Total: 6]


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [1] [An Overview of the JPEG AI Learning-Based Image Coding Standard](https://arxiv.org/abs/2510.13867)
*Semih Esenlik,Yaojun Wu,Zhaobin Zhang,Ye-Kui Wang,Kai Zhang,Li Zhang,João Ascenso,Shan Liu*

Main category: eess.IV

TL;DR: JPEG AI是JPEG组织正在开发的基于学习的图像编码新标准，专注于为人类视觉和机器处理提供单流紧凑压缩表示，相比现有标准在多种质量指标上显著降低BD-rate，计划2025年初完成。


<details>
  <summary>Details</summary>
Motivation: 创建实用的基于学习的图像编码标准，提供单流紧凑压缩域表示，同时满足人类视觉化和机器处理需求，确保广泛的互操作性。

Method: 采用学习-based图像编码方法，设计支持在各种设备和应用上部署的技术特性，包括多种质量评估指标。

Result: 相比现有标准，在MS-SSIM、FSIM、VIF、VMAF、PSNR-HVS、IW-SSIM和NLPD等质量指标上实现了显著的BD-rate降低。

Conclusion: JPEG AI标准在图像编码领域展现出巨大潜力，为人类视觉和机器处理提供了高效的压缩解决方案，具有广泛的应用前景。

Abstract: JPEG AI is an emerging learning-based image coding standard developed by
Joint Photographic Experts Group (JPEG). The scope of the JPEG AI is the
creation of a practical learning-based image coding standard offering a
single-stream, compact compressed domain representation, targeting both human
visualization and machine consumption. Scheduled for completion in early 2025,
the first version of JPEG AI focuses on human vision tasks, demonstrating
significant BD-rate reductions compared to existing standards, in terms of
MS-SSIM, FSIM, VIF, VMAF, PSNR-HVS, IW-SSIM and NLPD quality metrics. Designed
to ensure broad interoperability, JPEG AI incorporates various design features
to support deployment across diverse devices and applications. This paper
provides an overview of the technical features and characteristics of the JPEG
AI standard.

</details>


### [2] [Incomplete Multi-view Clustering via Hierarchical Semantic Alignment and Cooperative Completion](https://arxiv.org/abs/2510.13887)
*Xiaojian Ding,Lin Zhao,Xian Li,Xiaoying Zhu*

Main category: eess.IV

TL;DR: 提出了一种基于分层语义对齐和协同补全的不完整多视图聚类框架HSACC，通过双级语义空间设计实现鲁棒的跨视图融合，并在多个基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决不完整多视图数据中某些样本完全缺失某些视图的问题，现有深度不完整多视图聚类方法依赖静态融合策略或两阶段流程，导致次优融合结果和错误传播问题。

Method: HSACC框架采用双级语义空间设计：低层语义空间通过最大化跨视图互信息确保一致性对齐；高层语义空间基于个体视图与初始融合表示之间的分布亲和性动态分配自适应视图权重，然后进行加权融合生成统一全局表示。同时通过将对齐的潜在表示投影到高维语义空间来隐式恢复缺失视图，并联合优化重构和聚类目标。

Result: 在五个基准数据集上的实验结果表明，HSACC显著优于最先进的方法。消融研究验证了分层对齐和动态加权机制的有效性，参数分析确认了模型对超参数变化的鲁棒性。

Conclusion: HSACC通过分层语义对齐和协同补全机制，有效解决了不完整多视图聚类中的融合和补全问题，实现了优异的聚类性能。

Abstract: Incomplete multi-view data, where certain views are entirely missing for some
samples, poses significant challenges for traditional multi-view clustering
methods. Existing deep incomplete multi-view clustering approaches often rely
on static fusion strategies or two-stage pipelines, leading to suboptimal
fusion results and error propagation issues. To address these limitations, this
paper proposes a novel incomplete multi-view clustering framework based on
Hierarchical Semantic Alignment and Cooperative Completion (HSACC). HSACC
achieves robust cross-view fusion through a dual-level semantic space design.
In the low-level semantic space, consistency alignment is ensured by maximizing
mutual information across views. In the high-level semantic space, adaptive
view weights are dynamically assigned based on the distributional affinity
between individual views and an initial fused representation, followed by
weighted fusion to generate a unified global representation. Additionally,
HSACC implicitly recovers missing views by projecting aligned latent
representations into high-dimensional semantic spaces and jointly optimizes
reconstruction and clustering objectives, enabling cooperative learning of
completion and clustering. Experimental results demonstrate that HSACC
significantly outperforms state-of-the-art methods on five benchmark datasets.
Ablation studies validate the effectiveness of the hierarchical alignment and
dynamic weighting mechanisms, while parameter analysis confirms the model's
robustness to hyperparameter variations.

</details>


### [3] [Millimeter Wave Inverse Pinhole Imaging](https://arxiv.org/abs/2510.13904)
*Akarsh Prabhakara,Yawen Liu,Aswin C. Sankaranarayanan,Anthony Rowe,Swarun Kumar*

Main category: eess.IV

TL;DR: Umbra系统通过旋转毫米波"逆针孔"技术，在静态安装的紧凑型毫米波雷达上实现了5倍的角度分辨率提升，从14°提高到2.5°，特别适用于无人机等动态场景。


<details>
  <summary>Details</summary>
Motivation: 解决静态安装的紧凑型毫米波雷达因尺寸限制导致角度分辨率不足的问题，特别是在无人机悬停等动态场景中。

Method: 提出旋转毫米波"逆针孔"概念，利用轻量级设计实现低功耗旋转，并发现飞行器螺旋桨可充当天然逆针孔。

Result: Umbra系统仅用单个天线就能实现2.5°的角度分辨率，相比基线紧凑毫米波雷达的14°提升了5倍。

Conclusion: 逆针孔技术能有效提升紧凑型毫米波雷达的角度分辨率，为静态安装雷达提供高分辨率成像能力，在无人机等应用中具有重要价值。

Abstract: Millimeter wave (mmWave) radars are popular for perception in vision-denied
contexts due to their compact size. This paper explores emerging use-cases that
involve static mount or momentarily-static compact radars, for example, a
hovering drone. The key challenge with static compact radars is that their
limited form-factor also limits their angular resolution. This paper presents
Umbra, a mmWave high resolution imaging system, that introduces the concept of
rotating mmWave "inverse pinholes" for angular resolution enhancement. We
present the imaging system model, design, and evaluation of mmWave inverse
pinholes. The inverse pinhole is attractive for its lightweight nature, which
enables low-power rotation, upgrading static-mount radars. We also show how
propellers in aerial vehicles act as natural inverse pinholes and can enjoy the
benefits of high-resolution imaging even while they are momentarily static,
e.g., hovering. Our evaluation shows Umbra resolving up to 2.5$^{\circ}$ with
just a single antenna, a 5$\times$ improvement compared to 14$^{\circ}$ from a
compact mmWave radar baseline.

</details>


### [4] [Reinforcement Learning for Unsupervised Domain Adaptation in Spatio-Temporal Echocardiography Segmentation](https://arxiv.org/abs/2510.14244)
*Arnaud Judge,Nicolas Duchateau,Thierry Judge,Roman A. Sandler,Joseph Z. Sokol,Christian Desrosiers,Olivier Bernard,Pierre-Marc Jodoin*

Main category: eess.IV

TL;DR: RL4Seg3D是一个用于2D+时间超声心动图分割的无监督域自适应框架，通过强化学习提高分割准确性、解剖有效性和时间一致性，无需目标域标签。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分割中域自适应方法在目标域可靠性不足的问题，特别是在时空数据和超声心动图中，噪声和伪影会进一步降低分割性能。

Method: 集成新颖的奖励函数和融合方案，利用强化学习进行图像分割，处理完整尺寸输入视频，提高关键地标精度。

Result: 在超过30,000个超声心动图视频上验证，优于标准域自适应技术，无需目标域标签。

Conclusion: RL4Seg3D框架有效提升了超声心动图分割的准确性、解剖有效性和时间一致性，并提供了鲁棒的不确定性估计器。

Abstract: Domain adaptation methods aim to bridge the gap between datasets by enabling
knowledge transfer across domains, reducing the need for additional expert
annotations. However, many approaches struggle with reliability in the target
domain, an issue particularly critical in medical image segmentation, where
accuracy and anatomical validity are essential. This challenge is further
exacerbated in spatio-temporal data, where the lack of temporal consistency can
significantly degrade segmentation quality, and particularly in
echocardiography, where the presence of artifacts and noise can further hinder
segmentation performance. To address these issues, we present RL4Seg3D, an
unsupervised domain adaptation framework for 2D + time echocardiography
segmentation. RL4Seg3D integrates novel reward functions and a fusion scheme to
enhance key landmark precision in its segmentations while processing full-sized
input videos. By leveraging reinforcement learning for image segmentation, our
approach improves accuracy, anatomical validity, and temporal consistency while
also providing, as a beneficial side effect, a robust uncertainty estimator,
which can be used at test time to further enhance segmentation performance. We
demonstrate the effectiveness of our framework on over 30,000 echocardiographic
videos, showing that it outperforms standard domain adaptation techniques
without the need for any labels on the target domain. Code is available at
https://github.com/arnaudjudge/RL4Seg3D.

</details>


### [5] [A Density-Informed Multimodal Artificial Intelligence Framework for Improving Breast Cancer Detection Across All Breast Densities](https://arxiv.org/abs/2510.14340)
*Siva Teja Kakileti,Bharath Govindaraju,Sudhakar Sampangi,Geetha Manjunath*

Main category: eess.IV

TL;DR: 该研究提出了一种基于乳腺密度的多模态AI框架，结合乳腺X线摄影和热成像技术，根据乳腺组织密度动态选择最佳成像方式，显著提高了乳腺癌检测的敏感性。


<details>
  <summary>Details</summary>
Motivation: 传统乳腺X线摄影在致密乳腺组织中的敏感性较低，导致漏诊或延迟诊断。需要开发能够补充乳腺X线结构数据的功能性成像方法。

Method: 324名女性同时接受乳腺X线摄影和热成像检查。使用多视角深度学习模型分析乳腺X线图像，通过血管和热放射组学分析热成像图像。基于乳腺密度动态选择最佳成像方式：脂肪型乳腺使用乳腺X线AI，致密型乳腺使用热成像AI。

Result: 多模态AI框架敏感性达94.55%，特异性79.93%，优于单独使用乳腺X线AI（敏感性81.82%，特异性86.25%）或热成像AI（敏感性92.73%，特异性75.46%）。乳腺X线在致密乳腺中敏感性显著下降至67.86%，而热成像AI在两种乳腺类型中均保持高敏感性（92.59%和92.86%）。

Conclusion: 基于密度的多模态AI框架能够克服单模态筛查的关键限制，在不同乳腺组织中均提供高性能。该框架具有可解释性、低成本、易于部署的特点，为改善乳腺癌筛查结果提供了实用路径。

Abstract: Mammography, the current standard for breast cancer screening, has reduced
sensitivity in women with dense breast tissue, contributing to missed or
delayed diagnoses. Thermalytix, an AI-based thermal imaging modality, captures
functional vascular and metabolic cues that may complement mammographic
structural data. This study investigates whether a breast density-informed
multi-modal AI framework can improve cancer detection by dynamically selecting
the appropriate imaging modality based on breast tissue composition. A total of
324 women underwent both mammography and thermal imaging. Mammography images
were analyzed using a multi-view deep learning model, while Thermalytix
assessed thermal images through vascular and thermal radiomics. The proposed
framework utilized Mammography AI for fatty breasts and Thermalytix AI for
dense breasts, optimizing predictions based on tissue type. This multi-modal AI
framework achieved a sensitivity of 94.55% (95% CI: 88.54-100) and specificity
of 79.93% (95% CI: 75.14-84.71), outperforming standalone mammography AI
(sensitivity 81.82%, specificity 86.25%) and Thermalytix AI (sensitivity
92.73%, specificity 75.46%). Importantly, the sensitivity of Mammography
dropped significantly in dense breasts (67.86%) versus fatty breasts (96.30%),
whereas Thermalytix AI maintained high and consistent sensitivity in both
(92.59% and 92.86%, respectively). This demonstrates that a density-informed
multi-modal AI framework can overcome key limitations of unimodal screening and
deliver high performance across diverse breast compositions. The proposed
framework is interpretable, low-cost, and easily deployable, offering a
practical path to improving breast cancer screening outcomes in both
high-resource and resource-limited settings.

</details>


### [6] [EdgeNavMamba: Mamba Optimized Object Detection for Energy Efficient Edge Devices](https://arxiv.org/abs/2510.14946)
*Romina Aalishah,Mozhgan Navardi,Tinoosh Mohsenin*

Main category: eess.IV

TL;DR: EdgeNavMamba是一个基于强化学习的导航框架，使用高效的Mamba目标检测模型在资源受限的边缘设备上实现实时目标导向导航，显著减少了模型大小和能耗。


<details>
  <summary>Details</summary>
Motivation: 边缘设备计算能力和内存有限，需要高效和压缩的深度学习模型来实现自主导航的实时应用。

Method: 提出EdgeNavMamba框架，使用Mamba目标检测模型作为预处理模块提取视觉输入的边界框，然后传递给RL策略控制目标导向导航；创建了自定义形状检测数据集进行训练和评估。

Result: 学生模型在NVIDIA Jetson Orin Nano和Raspberry Pi 5上实现了67%的大小减少和高达73%的每推理能耗降低，同时保持与教师模型相同的性能；在MiniWorld和IsaacLab模拟器中保持高检测精度，参数比基线减少31%；在MiniWorld模拟器中导航策略在不同复杂度的环境中达到超过90%的成功率。

Conclusion: EdgeNavMamba框架在资源受限的边缘设备上实现了高效准确的目标导向导航，显著提升了模型效率和能耗表现。

Abstract: Deployment of efficient and accurate Deep Learning models has long been a
challenge in autonomous navigation, particularly for real-time applications on
resource-constrained edge devices. Edge devices are limited in computing power
and memory, making model efficiency and compression essential. In this work, we
propose EdgeNavMamba, a reinforcement learning-based framework for
goal-directed navigation using an efficient Mamba object detection model. To
train and evaluate the detector, we introduce a custom shape detection dataset
collected in diverse indoor settings, reflecting visual cues common in
real-world navigation. The object detector serves as a pre-processing module,
extracting bounding boxes (BBOX) from visual input, which are then passed to an
RL policy to control goal-oriented navigation. Experimental results show that
the student model achieved a reduction of 67% in size, and up to 73% in energy
per inference on edge devices of NVIDIA Jetson Orin Nano and Raspberry Pi 5,
while keeping the same performance as the teacher model. EdgeNavMamba also
maintains high detection accuracy in MiniWorld and IsaacLab simulators while
reducing parameters by 31% compared to the baseline. In the MiniWorld
simulator, the navigation policy achieves over 90% success across environments
of varying complexity.

</details>

<div id=toc></div>

# Table of Contents

- [eess.IV](#eess.IV) [Total: 6]


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [1] [An Overview of the JPEG AI Learning-Based Image Coding Standard](https://arxiv.org/abs/2510.13867)
*Semih Esenlik,Yaojun Wu,Zhaobin Zhang,Ye-Kui Wang,Kai Zhang,Li Zhang,João Ascenso,Shan Liu*

Main category: eess.IV

TL;DR: JPEG AI是JPEG组织正在开发的新一代基于学习的图像编码标准，旨在为人类视觉和机器分析提供统一的压缩表示，相比现有标准在多种质量指标上显著降低比特率。


<details>
  <summary>Details</summary>
Motivation: 创建实用的基于学习的图像编码标准，提供单一流、紧凑的压缩域表示，同时面向人类可视化和机器消费需求。

Method: 采用学习型编码方法，结合多种设计特性以确保广泛的互操作性，支持在不同设备和应用中的部署。

Result: 相比现有标准，在MS-SSIM、FSIM、VIF、VMAF、PSNR-HVS、IW-SSIM和NLPD等多种质量指标上实现了显著的BD-rate降低。

Conclusion: JPEG AI标准预计于2025年初完成，其首个版本专注于人类视觉任务，具备强大的技术特性和广泛的应用前景。

Abstract: JPEG AI is an emerging learning-based image coding standard developed by
Joint Photographic Experts Group (JPEG). The scope of the JPEG AI is the
creation of a practical learning-based image coding standard offering a
single-stream, compact compressed domain representation, targeting both human
visualization and machine consumption. Scheduled for completion in early 2025,
the first version of JPEG AI focuses on human vision tasks, demonstrating
significant BD-rate reductions compared to existing standards, in terms of
MS-SSIM, FSIM, VIF, VMAF, PSNR-HVS, IW-SSIM and NLPD quality metrics. Designed
to ensure broad interoperability, JPEG AI incorporates various design features
to support deployment across diverse devices and applications. This paper
provides an overview of the technical features and characteristics of the JPEG
AI standard.

</details>


### [2] [Incomplete Multi-view Clustering via Hierarchical Semantic Alignment and Cooperative Completion](https://arxiv.org/abs/2510.13887)
*Xiaojian Ding,Lin Zhao,Xian Li,Xiaoying Zhu*

Main category: eess.IV

TL;DR: 提出了一种基于层次语义对齐和协同补全的不完整多视图聚类框架HSACC，通过双级语义空间设计实现稳健的跨视图融合，在多个基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决不完整多视图数据中某些样本完全缺失某些视图的问题，现有深度不完整多视图聚类方法依赖静态融合策略或两阶段流程，导致次优融合结果和误差传播问题。

Method: HSACC框架采用双级语义空间设计：在低层语义空间通过最大化跨视图互信息确保一致性对齐；在高层语义空间基于单个视图与初始融合表示之间的分布亲和度动态分配自适应视图权重，然后进行加权融合生成统一全局表示。同时通过将对齐的潜在表示投影到高维语义空间来隐式恢复缺失视图，并联合优化重构和聚类目标。

Result: 在五个基准数据集上的实验结果表明，HSACC显著优于最先进的方法。消融研究验证了层次对齐和动态加权机制的有效性，参数分析证实了模型对超参数变化的鲁棒性。

Conclusion: HSACC通过层次语义对齐和协同补全机制有效解决了不完整多视图聚类问题，实现了稳健的跨视图融合和缺失视图恢复，在多个数据集上表现出优越性能。

Abstract: Incomplete multi-view data, where certain views are entirely missing for some
samples, poses significant challenges for traditional multi-view clustering
methods. Existing deep incomplete multi-view clustering approaches often rely
on static fusion strategies or two-stage pipelines, leading to suboptimal
fusion results and error propagation issues. To address these limitations, this
paper proposes a novel incomplete multi-view clustering framework based on
Hierarchical Semantic Alignment and Cooperative Completion (HSACC). HSACC
achieves robust cross-view fusion through a dual-level semantic space design.
In the low-level semantic space, consistency alignment is ensured by maximizing
mutual information across views. In the high-level semantic space, adaptive
view weights are dynamically assigned based on the distributional affinity
between individual views and an initial fused representation, followed by
weighted fusion to generate a unified global representation. Additionally,
HSACC implicitly recovers missing views by projecting aligned latent
representations into high-dimensional semantic spaces and jointly optimizes
reconstruction and clustering objectives, enabling cooperative learning of
completion and clustering. Experimental results demonstrate that HSACC
significantly outperforms state-of-the-art methods on five benchmark datasets.
Ablation studies validate the effectiveness of the hierarchical alignment and
dynamic weighting mechanisms, while parameter analysis confirms the model's
robustness to hyperparameter variations.

</details>


### [3] [Millimeter Wave Inverse Pinhole Imaging](https://arxiv.org/abs/2510.13904)
*Akarsh Prabhakara,Yawen Liu,Aswin C. Sankaranarayanan,Anthony Rowe,Swarun Kumar*

Main category: eess.IV

TL;DR: Umbra系统通过旋转毫米波"逆针孔"概念提升静态紧凑雷达的角分辨率，在单天线情况下实现2.5°分辨率，比基线14°提升5倍。


<details>
  <summary>Details</summary>
Motivation: 解决静态紧凑毫米波雷达因尺寸限制导致的角分辨率不足问题，特别是在无人机悬停等场景中。

Method: 提出旋转毫米波"逆针孔"概念，利用轻量级设计实现低功耗旋转，并发现无人机螺旋桨可作为天然逆针孔。

Result: Umbra系统仅用单天线即可实现2.5°角分辨率，相比紧凑毫米波雷达基线的14°分辨率提升了5倍。

Conclusion: 逆针孔技术为静态安装雷达提供了有效的角分辨率增强方案，特别适合无人机等轻量级应用场景。

Abstract: Millimeter wave (mmWave) radars are popular for perception in vision-denied
contexts due to their compact size. This paper explores emerging use-cases that
involve static mount or momentarily-static compact radars, for example, a
hovering drone. The key challenge with static compact radars is that their
limited form-factor also limits their angular resolution. This paper presents
Umbra, a mmWave high resolution imaging system, that introduces the concept of
rotating mmWave "inverse pinholes" for angular resolution enhancement. We
present the imaging system model, design, and evaluation of mmWave inverse
pinholes. The inverse pinhole is attractive for its lightweight nature, which
enables low-power rotation, upgrading static-mount radars. We also show how
propellers in aerial vehicles act as natural inverse pinholes and can enjoy the
benefits of high-resolution imaging even while they are momentarily static,
e.g., hovering. Our evaluation shows Umbra resolving up to 2.5$^{\circ}$ with
just a single antenna, a 5$\times$ improvement compared to 14$^{\circ}$ from a
compact mmWave radar baseline.

</details>


### [4] [Reinforcement Learning for Unsupervised Domain Adaptation in Spatio-Temporal Echocardiography Segmentation](https://arxiv.org/abs/2510.14244)
*Arnaud Judge,Nicolas Duchateau,Thierry Judge,Roman A. Sandler,Joseph Z. Sokol,Christian Desrosiers,Olivier Bernard,Pierre-Marc Jodoin*

Main category: eess.IV

TL;DR: RL4Seg3D是一个用于2D+时间超声心动图分割的无监督域适应框架，通过强化学习和新颖的奖励函数提高分割精度、解剖有效性和时间一致性，无需目标域标注。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分割中域适应方法的可靠性问题，特别是在超声心动图这种存在伪影和噪声的时空数据中，传统方法缺乏时间一致性和解剖有效性。

Method: 提出RL4Seg3D框架，集成新颖的奖励函数和融合方案，利用强化学习处理全尺寸输入视频，提高关键地标精度。

Result: 在超过30,000个超声心动图视频上验证，性能优于标准域适应技术，无需目标域标签，并提供鲁棒的不确定性估计器。

Conclusion: RL4Seg3D有效解决了医学图像分割中的域适应挑战，提高了分割质量和时间一致性，为临床应用提供了可靠解决方案。

Abstract: Domain adaptation methods aim to bridge the gap between datasets by enabling
knowledge transfer across domains, reducing the need for additional expert
annotations. However, many approaches struggle with reliability in the target
domain, an issue particularly critical in medical image segmentation, where
accuracy and anatomical validity are essential. This challenge is further
exacerbated in spatio-temporal data, where the lack of temporal consistency can
significantly degrade segmentation quality, and particularly in
echocardiography, where the presence of artifacts and noise can further hinder
segmentation performance. To address these issues, we present RL4Seg3D, an
unsupervised domain adaptation framework for 2D + time echocardiography
segmentation. RL4Seg3D integrates novel reward functions and a fusion scheme to
enhance key landmark precision in its segmentations while processing full-sized
input videos. By leveraging reinforcement learning for image segmentation, our
approach improves accuracy, anatomical validity, and temporal consistency while
also providing, as a beneficial side effect, a robust uncertainty estimator,
which can be used at test time to further enhance segmentation performance. We
demonstrate the effectiveness of our framework on over 30,000 echocardiographic
videos, showing that it outperforms standard domain adaptation techniques
without the need for any labels on the target domain. Code is available at
https://github.com/arnaudjudge/RL4Seg3D.

</details>


### [5] [A Density-Informed Multimodal Artificial Intelligence Framework for Improving Breast Cancer Detection Across All Breast Densities](https://arxiv.org/abs/2510.14340)
*Siva Teja Kakileti,Bharath Govindaraju,Sudhakar Sampangi,Geetha Manjunath*

Main category: eess.IV

TL;DR: 该研究提出了一种基于乳腺密度信息的多模态AI框架，结合乳腺X线摄影和热成像技术，根据乳腺组织密度动态选择最佳成像模式，显著提高了乳腺癌检测的敏感性，特别是在致密乳腺组织中的检测性能。


<details>
  <summary>Details</summary>
Motivation: 乳腺X线摄影在致密乳腺组织中的敏感性降低，导致漏诊或延迟诊断。热成像技术能够捕捉功能性的血管和代谢线索，可能补充乳腺X线摄影的结构数据，从而提高检测准确性。

Method: 324名女性同时接受乳腺X线摄影和热成像检查。使用多视图深度学习模型分析乳腺X线图像，通过血管和热放射组学评估热成像图像。提出的框架根据乳腺组织密度动态选择成像模式：脂肪型乳腺使用乳腺X线AI，致密型乳腺使用Thermalytix AI。

Result: 多模态AI框架实现了94.55%的敏感性和79.93%的特异性，优于单独的乳腺X线AI（敏感性81.82%，特异性86.25%）和Thermalytix AI（敏感性92.73%，特异性75.46%）。乳腺X线在致密乳腺中的敏感性显著下降至67.86%，而Thermalytix AI在两种乳腺类型中均保持高敏感性（92.59%和92.86%）。

Conclusion: 基于乳腺密度信息的多模态AI框架能够克服单模态筛查的关键限制，在不同乳腺组织中提供高性能检测。该框架具有可解释性、低成本和易于部署的特点，为提高乳腺癌筛查结果提供了实用途径。

Abstract: Mammography, the current standard for breast cancer screening, has reduced
sensitivity in women with dense breast tissue, contributing to missed or
delayed diagnoses. Thermalytix, an AI-based thermal imaging modality, captures
functional vascular and metabolic cues that may complement mammographic
structural data. This study investigates whether a breast density-informed
multi-modal AI framework can improve cancer detection by dynamically selecting
the appropriate imaging modality based on breast tissue composition. A total of
324 women underwent both mammography and thermal imaging. Mammography images
were analyzed using a multi-view deep learning model, while Thermalytix
assessed thermal images through vascular and thermal radiomics. The proposed
framework utilized Mammography AI for fatty breasts and Thermalytix AI for
dense breasts, optimizing predictions based on tissue type. This multi-modal AI
framework achieved a sensitivity of 94.55% (95% CI: 88.54-100) and specificity
of 79.93% (95% CI: 75.14-84.71), outperforming standalone mammography AI
(sensitivity 81.82%, specificity 86.25%) and Thermalytix AI (sensitivity
92.73%, specificity 75.46%). Importantly, the sensitivity of Mammography
dropped significantly in dense breasts (67.86%) versus fatty breasts (96.30%),
whereas Thermalytix AI maintained high and consistent sensitivity in both
(92.59% and 92.86%, respectively). This demonstrates that a density-informed
multi-modal AI framework can overcome key limitations of unimodal screening and
deliver high performance across diverse breast compositions. The proposed
framework is interpretable, low-cost, and easily deployable, offering a
practical path to improving breast cancer screening outcomes in both
high-resource and resource-limited settings.

</details>


### [6] [EdgeNavMamba: Mamba Optimized Object Detection for Energy Efficient Edge Devices](https://arxiv.org/abs/2510.14946)
*Romina Aalishah,Mozhgan Navardi,Tinoosh Mohsenin*

Main category: eess.IV

TL;DR: EdgeNavMamba是一个基于强化学习的导航框架，使用高效的Mamba目标检测模型在资源受限的边缘设备上实现目标导向导航，显著减少了模型大小和能耗，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 边缘设备计算能力和内存有限，需要高效且压缩的深度学习模型来支持自主导航的实时应用。

Method: 提出EdgeNavMamba框架，使用Mamba目标检测模型作为预处理模块提取视觉输入的边界框，然后传递给RL策略控制目标导向导航。创建了自定义形状检测数据集进行训练和评估。

Result: 学生模型在NVIDIA Jetson Orin Nano和Raspberry Pi 5上实现了67%的大小减少和高达73%的每次推理能耗降低，性能与教师模型相当。在MiniWorld和IsaacLab模拟器中保持高检测精度，参数比基线减少31%。在MiniWorld模拟器中，导航策略在不同复杂度的环境中成功率超过90%。

Conclusion: EdgeNavMamba框架在资源受限的边缘设备上有效实现了高效准确的目标导向导航，显著提升了模型效率和能耗表现。

Abstract: Deployment of efficient and accurate Deep Learning models has long been a
challenge in autonomous navigation, particularly for real-time applications on
resource-constrained edge devices. Edge devices are limited in computing power
and memory, making model efficiency and compression essential. In this work, we
propose EdgeNavMamba, a reinforcement learning-based framework for
goal-directed navigation using an efficient Mamba object detection model. To
train and evaluate the detector, we introduce a custom shape detection dataset
collected in diverse indoor settings, reflecting visual cues common in
real-world navigation. The object detector serves as a pre-processing module,
extracting bounding boxes (BBOX) from visual input, which are then passed to an
RL policy to control goal-oriented navigation. Experimental results show that
the student model achieved a reduction of 67% in size, and up to 73% in energy
per inference on edge devices of NVIDIA Jetson Orin Nano and Raspberry Pi 5,
while keeping the same performance as the teacher model. EdgeNavMamba also
maintains high detection accuracy in MiniWorld and IsaacLab simulators while
reducing parameters by 31% compared to the baseline. In the MiniWorld
simulator, the navigation policy achieves over 90% success across environments
of varying complexity.

</details>

{"id": "2510.13867", "categories": ["eess.IV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.13867", "abs": "https://arxiv.org/abs/2510.13867", "authors": ["Semih Esenlik", "Yaojun Wu", "Zhaobin Zhang", "Ye-Kui Wang", "Kai Zhang", "Li Zhang", "Jo√£o Ascenso", "Shan Liu"], "title": "An Overview of the JPEG AI Learning-Based Image Coding Standard", "comment": "IEEE Transactions on Circuits and Systems for Video Technology", "summary": "JPEG AI is an emerging learning-based image coding standard developed by\nJoint Photographic Experts Group (JPEG). The scope of the JPEG AI is the\ncreation of a practical learning-based image coding standard offering a\nsingle-stream, compact compressed domain representation, targeting both human\nvisualization and machine consumption. Scheduled for completion in early 2025,\nthe first version of JPEG AI focuses on human vision tasks, demonstrating\nsignificant BD-rate reductions compared to existing standards, in terms of\nMS-SSIM, FSIM, VIF, VMAF, PSNR-HVS, IW-SSIM and NLPD quality metrics. Designed\nto ensure broad interoperability, JPEG AI incorporates various design features\nto support deployment across diverse devices and applications. This paper\nprovides an overview of the technical features and characteristics of the JPEG\nAI standard."}
{"id": "2510.13887", "categories": ["eess.IV", "cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.13887", "abs": "https://arxiv.org/abs/2510.13887", "authors": ["Xiaojian Ding", "Lin Zhao", "Xian Li", "Xiaoying Zhu"], "title": "Incomplete Multi-view Clustering via Hierarchical Semantic Alignment and Cooperative Completion", "comment": null, "summary": "Incomplete multi-view data, where certain views are entirely missing for some\nsamples, poses significant challenges for traditional multi-view clustering\nmethods. Existing deep incomplete multi-view clustering approaches often rely\non static fusion strategies or two-stage pipelines, leading to suboptimal\nfusion results and error propagation issues. To address these limitations, this\npaper proposes a novel incomplete multi-view clustering framework based on\nHierarchical Semantic Alignment and Cooperative Completion (HSACC). HSACC\nachieves robust cross-view fusion through a dual-level semantic space design.\nIn the low-level semantic space, consistency alignment is ensured by maximizing\nmutual information across views. In the high-level semantic space, adaptive\nview weights are dynamically assigned based on the distributional affinity\nbetween individual views and an initial fused representation, followed by\nweighted fusion to generate a unified global representation. Additionally,\nHSACC implicitly recovers missing views by projecting aligned latent\nrepresentations into high-dimensional semantic spaces and jointly optimizes\nreconstruction and clustering objectives, enabling cooperative learning of\ncompletion and clustering. Experimental results demonstrate that HSACC\nsignificantly outperforms state-of-the-art methods on five benchmark datasets.\nAblation studies validate the effectiveness of the hierarchical alignment and\ndynamic weighting mechanisms, while parameter analysis confirms the model's\nrobustness to hyperparameter variations."}
{"id": "2510.13904", "categories": ["eess.IV", "cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.13904", "abs": "https://arxiv.org/abs/2510.13904", "authors": ["Akarsh Prabhakara", "Yawen Liu", "Aswin C. Sankaranarayanan", "Anthony Rowe", "Swarun Kumar"], "title": "Millimeter Wave Inverse Pinhole Imaging", "comment": null, "summary": "Millimeter wave (mmWave) radars are popular for perception in vision-denied\ncontexts due to their compact size. This paper explores emerging use-cases that\ninvolve static mount or momentarily-static compact radars, for example, a\nhovering drone. The key challenge with static compact radars is that their\nlimited form-factor also limits their angular resolution. This paper presents\nUmbra, a mmWave high resolution imaging system, that introduces the concept of\nrotating mmWave \"inverse pinholes\" for angular resolution enhancement. We\npresent the imaging system model, design, and evaluation of mmWave inverse\npinholes. The inverse pinhole is attractive for its lightweight nature, which\nenables low-power rotation, upgrading static-mount radars. We also show how\npropellers in aerial vehicles act as natural inverse pinholes and can enjoy the\nbenefits of high-resolution imaging even while they are momentarily static,\ne.g., hovering. Our evaluation shows Umbra resolving up to 2.5$^{\\circ}$ with\njust a single antenna, a 5$\\times$ improvement compared to 14$^{\\circ}$ from a\ncompact mmWave radar baseline."}
{"id": "2510.13933", "categories": ["eess.IV", "68T45 (Primary) 68T07, 68U05 (Secondary)", "I.3.7; I.2.10; I.4.8"], "pdf": "https://arxiv.org/pdf/2510.13933", "abs": "https://arxiv.org/abs/2510.13933", "authors": ["Tianxiang Yang", "Marco Volino", "Armin Mustafa", "Greg Maguire", "Robert Kosk"], "title": "Image-based Facial Rig Inversion", "comment": "The 22nd ACM SIGGRAPH European Conference on Visual Media Production\n  (CVMP2025) Short Paper", "summary": "We present an image-based rig inversion framework that leverages two\nmodalities: RGB appearance and RGB-encoded normal maps. Each modality is\nprocessed by an independent Hiera transformer backbone, and the extracted\nfeatures are fused to regress 102 rig parameters derived from the Facial Action\nCoding System (FACS). Experiments on synthetic and scanned datasets demonstrate\nthat the method generalizes to scanned data, producing faithful\nreconstructions."}
{"id": "2510.14244", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14244", "abs": "https://arxiv.org/abs/2510.14244", "authors": ["Arnaud Judge", "Nicolas Duchateau", "Thierry Judge", "Roman A. Sandler", "Joseph Z. Sokol", "Christian Desrosiers", "Olivier Bernard", "Pierre-Marc Jodoin"], "title": "Reinforcement Learning for Unsupervised Domain Adaptation in Spatio-Temporal Echocardiography Segmentation", "comment": "10 pages, submitted to IEEE TMI", "summary": "Domain adaptation methods aim to bridge the gap between datasets by enabling\nknowledge transfer across domains, reducing the need for additional expert\nannotations. However, many approaches struggle with reliability in the target\ndomain, an issue particularly critical in medical image segmentation, where\naccuracy and anatomical validity are essential. This challenge is further\nexacerbated in spatio-temporal data, where the lack of temporal consistency can\nsignificantly degrade segmentation quality, and particularly in\nechocardiography, where the presence of artifacts and noise can further hinder\nsegmentation performance. To address these issues, we present RL4Seg3D, an\nunsupervised domain adaptation framework for 2D + time echocardiography\nsegmentation. RL4Seg3D integrates novel reward functions and a fusion scheme to\nenhance key landmark precision in its segmentations while processing full-sized\ninput videos. By leveraging reinforcement learning for image segmentation, our\napproach improves accuracy, anatomical validity, and temporal consistency while\nalso providing, as a beneficial side effect, a robust uncertainty estimator,\nwhich can be used at test time to further enhance segmentation performance. We\ndemonstrate the effectiveness of our framework on over 30,000 echocardiographic\nvideos, showing that it outperforms standard domain adaptation techniques\nwithout the need for any labels on the target domain. Code is available at\nhttps://github.com/arnaudjudge/RL4Seg3D."}
{"id": "2510.14340", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14340", "abs": "https://arxiv.org/abs/2510.14340", "authors": ["Siva Teja Kakileti", "Bharath Govindaraju", "Sudhakar Sampangi", "Geetha Manjunath"], "title": "A Density-Informed Multimodal Artificial Intelligence Framework for Improving Breast Cancer Detection Across All Breast Densities", "comment": null, "summary": "Mammography, the current standard for breast cancer screening, has reduced\nsensitivity in women with dense breast tissue, contributing to missed or\ndelayed diagnoses. Thermalytix, an AI-based thermal imaging modality, captures\nfunctional vascular and metabolic cues that may complement mammographic\nstructural data. This study investigates whether a breast density-informed\nmulti-modal AI framework can improve cancer detection by dynamically selecting\nthe appropriate imaging modality based on breast tissue composition. A total of\n324 women underwent both mammography and thermal imaging. Mammography images\nwere analyzed using a multi-view deep learning model, while Thermalytix\nassessed thermal images through vascular and thermal radiomics. The proposed\nframework utilized Mammography AI for fatty breasts and Thermalytix AI for\ndense breasts, optimizing predictions based on tissue type. This multi-modal AI\nframework achieved a sensitivity of 94.55% (95% CI: 88.54-100) and specificity\nof 79.93% (95% CI: 75.14-84.71), outperforming standalone mammography AI\n(sensitivity 81.82%, specificity 86.25%) and Thermalytix AI (sensitivity\n92.73%, specificity 75.46%). Importantly, the sensitivity of Mammography\ndropped significantly in dense breasts (67.86%) versus fatty breasts (96.30%),\nwhereas Thermalytix AI maintained high and consistent sensitivity in both\n(92.59% and 92.86%, respectively). This demonstrates that a density-informed\nmulti-modal AI framework can overcome key limitations of unimodal screening and\ndeliver high performance across diverse breast compositions. The proposed\nframework is interpretable, low-cost, and easily deployable, offering a\npractical path to improving breast cancer screening outcomes in both\nhigh-resource and resource-limited settings."}
{"id": "2510.14946", "categories": ["eess.IV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14946", "abs": "https://arxiv.org/abs/2510.14946", "authors": ["Romina Aalishah", "Mozhgan Navardi", "Tinoosh Mohsenin"], "title": "EdgeNavMamba: Mamba Optimized Object Detection for Energy Efficient Edge Devices", "comment": "The 11th IEEE International Conference on Edge Computing and Scalable\n  Cloud (IEEE EdgeCom 2025)", "summary": "Deployment of efficient and accurate Deep Learning models has long been a\nchallenge in autonomous navigation, particularly for real-time applications on\nresource-constrained edge devices. Edge devices are limited in computing power\nand memory, making model efficiency and compression essential. In this work, we\npropose EdgeNavMamba, a reinforcement learning-based framework for\ngoal-directed navigation using an efficient Mamba object detection model. To\ntrain and evaluate the detector, we introduce a custom shape detection dataset\ncollected in diverse indoor settings, reflecting visual cues common in\nreal-world navigation. The object detector serves as a pre-processing module,\nextracting bounding boxes (BBOX) from visual input, which are then passed to an\nRL policy to control goal-oriented navigation. Experimental results show that\nthe student model achieved a reduction of 67% in size, and up to 73% in energy\nper inference on edge devices of NVIDIA Jetson Orin Nano and Raspberry Pi 5,\nwhile keeping the same performance as the teacher model. EdgeNavMamba also\nmaintains high detection accuracy in MiniWorld and IsaacLab simulators while\nreducing parameters by 31% compared to the baseline. In the MiniWorld\nsimulator, the navigation policy achieves over 90% success across environments\nof varying complexity."}
{"id": "2510.13887", "categories": ["eess.IV", "cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.13887", "abs": "https://arxiv.org/abs/2510.13887", "authors": ["Xiaojian Ding", "Lin Zhao", "Xian Li", "Xiaoying Zhu"], "title": "Incomplete Multi-view Clustering via Hierarchical Semantic Alignment and Cooperative Completion", "comment": null, "summary": "Incomplete multi-view data, where certain views are entirely missing for some\nsamples, poses significant challenges for traditional multi-view clustering\nmethods. Existing deep incomplete multi-view clustering approaches often rely\non static fusion strategies or two-stage pipelines, leading to suboptimal\nfusion results and error propagation issues. To address these limitations, this\npaper proposes a novel incomplete multi-view clustering framework based on\nHierarchical Semantic Alignment and Cooperative Completion (HSACC). HSACC\nachieves robust cross-view fusion through a dual-level semantic space design.\nIn the low-level semantic space, consistency alignment is ensured by maximizing\nmutual information across views. In the high-level semantic space, adaptive\nview weights are dynamically assigned based on the distributional affinity\nbetween individual views and an initial fused representation, followed by\nweighted fusion to generate a unified global representation. Additionally,\nHSACC implicitly recovers missing views by projecting aligned latent\nrepresentations into high-dimensional semantic spaces and jointly optimizes\nreconstruction and clustering objectives, enabling cooperative learning of\ncompletion and clustering. Experimental results demonstrate that HSACC\nsignificantly outperforms state-of-the-art methods on five benchmark datasets.\nAblation studies validate the effectiveness of the hierarchical alignment and\ndynamic weighting mechanisms, while parameter analysis confirms the model's\nrobustness to hyperparameter variations."}
{"id": "2510.14244", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14244", "abs": "https://arxiv.org/abs/2510.14244", "authors": ["Arnaud Judge", "Nicolas Duchateau", "Thierry Judge", "Roman A. Sandler", "Joseph Z. Sokol", "Christian Desrosiers", "Olivier Bernard", "Pierre-Marc Jodoin"], "title": "Reinforcement Learning for Unsupervised Domain Adaptation in Spatio-Temporal Echocardiography Segmentation", "comment": "10 pages, submitted to IEEE TMI", "summary": "Domain adaptation methods aim to bridge the gap between datasets by enabling\nknowledge transfer across domains, reducing the need for additional expert\nannotations. However, many approaches struggle with reliability in the target\ndomain, an issue particularly critical in medical image segmentation, where\naccuracy and anatomical validity are essential. This challenge is further\nexacerbated in spatio-temporal data, where the lack of temporal consistency can\nsignificantly degrade segmentation quality, and particularly in\nechocardiography, where the presence of artifacts and noise can further hinder\nsegmentation performance. To address these issues, we present RL4Seg3D, an\nunsupervised domain adaptation framework for 2D + time echocardiography\nsegmentation. RL4Seg3D integrates novel reward functions and a fusion scheme to\nenhance key landmark precision in its segmentations while processing full-sized\ninput videos. By leveraging reinforcement learning for image segmentation, our\napproach improves accuracy, anatomical validity, and temporal consistency while\nalso providing, as a beneficial side effect, a robust uncertainty estimator,\nwhich can be used at test time to further enhance segmentation performance. We\ndemonstrate the effectiveness of our framework on over 30,000 echocardiographic\nvideos, showing that it outperforms standard domain adaptation techniques\nwithout the need for any labels on the target domain. Code is available at\nhttps://github.com/arnaudjudge/RL4Seg3D."}
{"id": "2510.14340", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14340", "abs": "https://arxiv.org/abs/2510.14340", "authors": ["Siva Teja Kakileti", "Bharath Govindaraju", "Sudhakar Sampangi", "Geetha Manjunath"], "title": "A Density-Informed Multimodal Artificial Intelligence Framework for Improving Breast Cancer Detection Across All Breast Densities", "comment": null, "summary": "Mammography, the current standard for breast cancer screening, has reduced\nsensitivity in women with dense breast tissue, contributing to missed or\ndelayed diagnoses. Thermalytix, an AI-based thermal imaging modality, captures\nfunctional vascular and metabolic cues that may complement mammographic\nstructural data. This study investigates whether a breast density-informed\nmulti-modal AI framework can improve cancer detection by dynamically selecting\nthe appropriate imaging modality based on breast tissue composition. A total of\n324 women underwent both mammography and thermal imaging. Mammography images\nwere analyzed using a multi-view deep learning model, while Thermalytix\nassessed thermal images through vascular and thermal radiomics. The proposed\nframework utilized Mammography AI for fatty breasts and Thermalytix AI for\ndense breasts, optimizing predictions based on tissue type. This multi-modal AI\nframework achieved a sensitivity of 94.55% (95% CI: 88.54-100) and specificity\nof 79.93% (95% CI: 75.14-84.71), outperforming standalone mammography AI\n(sensitivity 81.82%, specificity 86.25%) and Thermalytix AI (sensitivity\n92.73%, specificity 75.46%). Importantly, the sensitivity of Mammography\ndropped significantly in dense breasts (67.86%) versus fatty breasts (96.30%),\nwhereas Thermalytix AI maintained high and consistent sensitivity in both\n(92.59% and 92.86%, respectively). This demonstrates that a density-informed\nmulti-modal AI framework can overcome key limitations of unimodal screening and\ndeliver high performance across diverse breast compositions. The proposed\nframework is interpretable, low-cost, and easily deployable, offering a\npractical path to improving breast cancer screening outcomes in both\nhigh-resource and resource-limited settings."}
{"id": "2510.14244", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14244", "abs": "https://arxiv.org/abs/2510.14244", "authors": ["Arnaud Judge", "Nicolas Duchateau", "Thierry Judge", "Roman A. Sandler", "Joseph Z. Sokol", "Christian Desrosiers", "Olivier Bernard", "Pierre-Marc Jodoin"], "title": "Reinforcement Learning for Unsupervised Domain Adaptation in Spatio-Temporal Echocardiography Segmentation", "comment": "10 pages, submitted to IEEE TMI", "summary": "Domain adaptation methods aim to bridge the gap between datasets by enabling\nknowledge transfer across domains, reducing the need for additional expert\nannotations. However, many approaches struggle with reliability in the target\ndomain, an issue particularly critical in medical image segmentation, where\naccuracy and anatomical validity are essential. This challenge is further\nexacerbated in spatio-temporal data, where the lack of temporal consistency can\nsignificantly degrade segmentation quality, and particularly in\nechocardiography, where the presence of artifacts and noise can further hinder\nsegmentation performance. To address these issues, we present RL4Seg3D, an\nunsupervised domain adaptation framework for 2D + time echocardiography\nsegmentation. RL4Seg3D integrates novel reward functions and a fusion scheme to\nenhance key landmark precision in its segmentations while processing full-sized\ninput videos. By leveraging reinforcement learning for image segmentation, our\napproach improves accuracy, anatomical validity, and temporal consistency while\nalso providing, as a beneficial side effect, a robust uncertainty estimator,\nwhich can be used at test time to further enhance segmentation performance. We\ndemonstrate the effectiveness of our framework on over 30,000 echocardiographic\nvideos, showing that it outperforms standard domain adaptation techniques\nwithout the need for any labels on the target domain. Code is available at\nhttps://github.com/arnaudjudge/RL4Seg3D."}
{"id": "2510.14340", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14340", "abs": "https://arxiv.org/abs/2510.14340", "authors": ["Siva Teja Kakileti", "Bharath Govindaraju", "Sudhakar Sampangi", "Geetha Manjunath"], "title": "A Density-Informed Multimodal Artificial Intelligence Framework for Improving Breast Cancer Detection Across All Breast Densities", "comment": null, "summary": "Mammography, the current standard for breast cancer screening, has reduced\nsensitivity in women with dense breast tissue, contributing to missed or\ndelayed diagnoses. Thermalytix, an AI-based thermal imaging modality, captures\nfunctional vascular and metabolic cues that may complement mammographic\nstructural data. This study investigates whether a breast density-informed\nmulti-modal AI framework can improve cancer detection by dynamically selecting\nthe appropriate imaging modality based on breast tissue composition. A total of\n324 women underwent both mammography and thermal imaging. Mammography images\nwere analyzed using a multi-view deep learning model, while Thermalytix\nassessed thermal images through vascular and thermal radiomics. The proposed\nframework utilized Mammography AI for fatty breasts and Thermalytix AI for\ndense breasts, optimizing predictions based on tissue type. This multi-modal AI\nframework achieved a sensitivity of 94.55% (95% CI: 88.54-100) and specificity\nof 79.93% (95% CI: 75.14-84.71), outperforming standalone mammography AI\n(sensitivity 81.82%, specificity 86.25%) and Thermalytix AI (sensitivity\n92.73%, specificity 75.46%). Importantly, the sensitivity of Mammography\ndropped significantly in dense breasts (67.86%) versus fatty breasts (96.30%),\nwhereas Thermalytix AI maintained high and consistent sensitivity in both\n(92.59% and 92.86%, respectively). This demonstrates that a density-informed\nmulti-modal AI framework can overcome key limitations of unimodal screening and\ndeliver high performance across diverse breast compositions. The proposed\nframework is interpretable, low-cost, and easily deployable, offering a\npractical path to improving breast cancer screening outcomes in both\nhigh-resource and resource-limited settings."}
